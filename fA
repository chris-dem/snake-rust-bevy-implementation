use std::sync::{Arc, OnceLock, RwLock};

use anyhow::Result as ARes;
use burn::{
    data::{
        dataloader::{DataLoader, batcher::*},
        dataset::vision::MnistItem,
    },
    prelude::*,
    tensor::ops::BoolTensor,
};
use itertools::Itertools;
// use itertools::Itertools;
use rand::prelude::*;
use snake_api_lib::{
    api::{GameAPI, GameAPIBinaryRepr},
    common::{Direction, GRID_X, GRID_Y},
    simulator::{PlayerTrait, SimulationStep, SimulationStepReward, Simulator},
};
use strum::IntoEnumIterator;

use crate::model::{Model, StateRepr};

#[derive(Debug)]
pub(crate) struct DatasetGenerator {
    gen_config: DatasetGeneratorConfig,
    rew_config: RewardConfig,
    simulator: Simulator,
}

#[derive(Debug, Clone, Copy)]
pub struct DatasetGeneratorConfig {
    pub number_eps: usize,
    pub ep_limit: usize,
    pub batch_size: usize,
}

#[derive(Debug, Clone, Copy)]
pub struct RewardConfig {
    pub step_rew: f32,
    pub fruit_rew: f32,
    pub win_rew: f32,
    pub lose_rew: f32,
}

#[derive(Clone, Debug)]
pub(crate) struct PlayerModel<'a, 'b, B: Backend> {
    model: &'a Model<B>,
    eps: f64,
    device: &'b B::Device,
    active_mode: bool,
}

impl<'a, 'b, B: Backend> PlayerModel<'a, 'b, B> {
    fn update_model<'c>(self, new_model: &'c Model<B>) -> PlayerModel<'c, 'b, B> {
        PlayerModel {
            model: new_model,
            ..self
        }
    }

    fn set_mode(&mut self, is_training: bool) {
        self.active_mode = is_training;
    }

    fn set_eps(&mut self, eps: f64) {
        self.eps = eps;
    }
}

impl<B: Backend> From<(GameAPIBinaryRepr, &B::Device)> for StateRepr<B> {
    fn from(value: (GameAPIBinaryRepr, &B::Device)) -> Self {
        let (GameAPIBinaryRepr(arr), dev) = value;
        let arr = arr
            .to_shape((1, GRID_X, GRID_Y))
            .expect("Padding with one should nof affect it");
        let arr = arr
            .as_standard_layout()
            .to_owned()
            .map(|x| *x as i32)
            .into_raw_vec_and_offset()
            .0;
        let td = TensorData::new(arr, [1, GRID_X, GRID_Y]);
        let td: Tensor<B, 4, Float> = Tensor::<B, 3, Int>::from_data(td, dev).one_hot(4).float();
        StateRepr(td)
    }
}

impl<'a, 'b, B: Backend> PlayerTrait for PlayerModel<'a, 'b, B> {
    fn choose_dir(&mut self, game_instance: &GameAPI, with_rng: &mut dyn RngCore) -> Direction {
        let dir = game_instance.snake.direction;
        let mut dir_vec = Direction::iter().enumerate().collect_vec();
        dir_vec.remove(dir.inverse() as usize);
        if self.active_mode && with_rng.random_bool(self.eps) {
            unimplemented!("Eps-greedy not implemented due to dropouts");
            // return dir_vec.choose(with_rng).unwrap().1;
        }
        let state_repr: StateRepr<B> = (game_instance.to_game_repr(), self.device).into();
        let m = Tensor::<B, 1, Int>::from_data(
            dir_vec
                .into_iter()
                .map(|x| x.0 as i32)
                .collect_array::<3>()
                .unwrap(),
            self.device,
        );
        let out: Tensor<B, 1> = self.model.forward(state_repr).flatten(0, 1).select(0, m);
        let indx = out.argmax(0).into_scalar().elem::<i64>();
        let indx = indx as usize;
        Direction::from(indx)
    }
}

impl DatasetGenerator {
    fn new(
        gen_config: DatasetGeneratorConfig,
        rew_config: RewardConfig,
        simulator: Simulator,
    ) -> Self {
        Self {
            gen_config,
            simulator,
            rew_config,
        }
    }

    fn iter_with_model<T, B: Backend>(
        &self,
        player: &mut PlayerModel<B>,
        with_rng: &mut T,
    ) -> impl Iterator<Item = BatchedSimulationStep<B>>
    where
        T: RngCore + Clone + SeedableRng + Send + Sync,
    {
        let p = self.gen_config.number_eps;
        let mut sims = (0..p)
            .map(|_| self.simulator.simulation(player, with_rng))
            .collect::<ARes<Vec<_>>>()
            .expect("Should compile")
            .into_iter()
            .flatten()
            .collect::<Vec<_>>();
        sims.shuffle(with_rng);
        // let prior_setting = player.active_mode;
        // player.active_mode = false;
        let batches = sims
            .chunks(self.gen_config.batch_size)
            .map(|c| self.batch_sims(c.iter().cloned(), player))
            .collect_vec();
        batches.into_iter()
    }

    fn batch_sims<B: Backend>(
        &self,
        els: impl Iterator<Item = SimulationStep>,
        player: &mut PlayerModel<B>,
    ) -> BatchedSimulationStep<B> {
        let mut v_snapshot: Vec<Tensor<B, 3, Float>> = vec![];
        let mut v_direction = vec![];
        let mut v_reward = vec![];
        let mut v_next_state_qual = vec![];
        for SimulationStep {
            snapshot,
            direction,
            reward,
            next_state,
        } in els
        {
            v_snapshot.push(Tensor::from_data(
                TensorData::new(snapshot.0.into_raw_vec_and_offset().0, [GRID_X, GRID_Y, 4]),
                player.device,
            ));
            v_direction.push(direction as i32);
            match reward {
                SimulationStepReward::Won => {
                    v_reward.push(self.rew_config.fruit_rew);
                    v_next_state_qual.push(self.rew_config.win_rew);
                }
                SimulationStepReward::Lost => {
                    v_reward.push(self.rew_config.step_rew);
                    v_next_state_qual.push(self.rew_config.lose_rew);
                }
                SimulationStepReward::Step => {
                    let st: StateRepr<B> = (
                        next_state.expect("Sohuld have next state in step"),
                        player.device,
                    )
                        .into();
                    v_reward.push(self.rew_config.step_rew);
                    let out = player.model.forward(st).argmax(0);
                    let el = out.max().into_scalar().elem::<f32>();
                    v_next_state_qual.push(el);
                }
                SimulationStepReward::Food => {
                    let st: StateRepr<B> = (
                        next_state.expect("Sohuld have next state in step"),
                        player.device,
                    )
                        .into();
                    let out = player.model.forward(st).argmax(0);
                    let el = out.max().into_scalar().elem::<f32>();
                    v_reward.push(self.rew_config.fruit_rew);
                    v_next_state_qual.push(el);
                }
            }
        }

        let b_size = v_direction.len();
        BatchedSimulationStep {
            snapshot: Tensor::stack(v_snapshot, 0),
            direction: Tensor::from_data(TensorData::new(v_direction, [b_size]), player.device),
            reward: Tensor::from_data(TensorData::new(v_reward, [b_size]), player.device),
            next_state_qual: Tensor::from_data(
                TensorData::new(v_next_state_qual, [b_size]),
                player.device,
            ),
        }
    }
}

pub struct BatchedSimulationStep<B: Backend> {
    pub snapshot: Tensor<B, 3, Float>,
    pub direction: Tensor<B, 1, Int>,
    pub reward: Tensor<B, 1, Float>,
    pub next_state_qual: Tensor<B, 1, Float>,
}
